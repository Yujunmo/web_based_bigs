{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea514cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë‚´ìš© ì½ê¸°\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import Docx2txtLoader, TextLoader\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import tempfile\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = UnstructuredLoader(\"./test_data/cash_flow.csv\")\n",
    "document = loader.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ìª¼ê°œê¸°\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1500    #  token ìˆ˜ \n",
    "    , chunk_overlap=200  #  ê²¹ì¹˜ëŠ” í† í° ìˆ˜ (ì¤‘ë³µë˜ëŠ” í† í°ì´ ë§ì„ìˆ˜ë¡ ë¬¸ë§¥ ìœ ì§€ì— ìœ ë¦¬í•˜ì§€ë§Œ, ì¤‘ë³µë˜ëŠ” í† í°ì´ ë§ì•„ì§ˆìˆ˜ë¡ ì €ì¥ê³µê°„ì´ ëŠ˜ì–´ë‚¨)\n",
    "    )\n",
    "\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4151cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ele in document_list:\n",
    "    for col in ele.metadata.keys():\n",
    "        if isinstance(ele.metadata[col], list):\n",
    "            ele.metadata[col] = \", \".join(map(str, ele.metadata[col]))\n",
    "        else:\n",
    "            ele.metadata[col] = str(ele.metadata[col]) if not isinstance(ele.metadata[col], (str, int, float, bool)) else ele.metadata[col]            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9789b153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './test_data/cash_flow.csv',\n",
       " 'file_directory': './test_data',\n",
       " 'filename': 'cash_flow.csv',\n",
       " 'last_modified': '2025-10-06T23:28:11',\n",
       " 'page_name': 'Sheet2',\n",
       " 'page_number': 1,\n",
       " 'text_as_html': '<table><tr><td>ìš´ìš©ì‚¬ì½”ë“œ</td><td>í€ë“œì½”ë“œ</td><td>í€ë“œëª…</td><td>ì¼ì</td><td>í˜„ê¸ˆíë¦„</td></tr><tr><td>03069</td><td>A0001</td><td>ì‚¼ì„±ì½”ë¦¬ì•„ì£¼ì‹í€ë“œ</td><td>2020-01-01</td><td>-10000</td></tr><tr><td>03069</td><td>A0001</td><td>ì‚¼ì„±ì½”ë¦¬ì•„ì£¼ì‹í€ë“œ</td><td>2020-07-01</td><td>-2000</td></tr><tr><td>03069</td><td>A0001</td><td>ì‚¼ì„±ì½”ë¦¬ì•„ì£¼ì‹í€ë“œ</td><td>2021-03-01</td><td>5000</td></tr><tr><td>03069</td><td>A0001</td><td>ì‚¼ì„±ì½”ë¦¬ì•„ì£¼ì‹í€ë“œ</td><td>2021-10-01</td><td>15000</td></tr><tr><td>03069</td><td>A0002</td><td>ì‚¼ì„±ê¸€ë¡œë²Œë°˜ë„ì²´í€ë“œ</td><td>2022-06-30</td><td>-10000000</td></tr><tr><td>03069</td><td>A0002</td><td>ì‚¼ì„±ê¸€ë¡œë²Œë°˜ë„ì²´í€ë“œ</td><td>2022-12-31</td><td>2000000</td></tr><tr><td>03069</td><td>A0002</td><td>ì‚¼ì„±ê¸€ë¡œë²Œë°˜ë„ì²´í€ë“œ</td><td>2023-12-31</td><td>2500000</td></tr><tr><td>03069</td><td>A0002</td><td>ì‚¼ì„±ê¸€ë¡œë²Œë°˜ë„ì²´í€ë“œ</td><td>2024-09-28</td><td>3000000</td></tr><tr><td>03069</td><td>A0002</td><td>ì‚¼ì„±ê¸€ë¡œë²Œë°˜ë„ì²´í€ë“œ</td><td>2024-12-31</td><td>3500000</td></tr><tr><td>03069</td><td>A0002</td><td>ì‚¼ì„±ê¸€ë¡œë²Œë°˜ë„ì²´í€ë“œ</td><td>2025-12-31</td><td>4000000</td></tr></table>',\n",
       " 'languages': 'kor',\n",
       " 'filetype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
       " 'category': 'Table',\n",
       " 'element_id': '1a9c5952194bd21c76dd1d099a48bdee'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_list[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f675d3fe",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì…ë ¥ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36a5bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83740b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel data ë²¡í„°í™”\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# embeddings = UpstageEmbeddings(model=\"embedding-query\")\n",
    "# database = Chroma(persist_directory='./public_db/' , collection_name='logic', embedding_function=embeddings)  # ê¸°ì¡´ì— ìƒì„±í•œ ë²¡í„° DBë¥¼ ë¶ˆëŸ¬ì˜´\n",
    "# print(database._collection.count())\n",
    "\n",
    "# # data í™•ì¸ \n",
    "# data = database.get()\n",
    "# print(data['documents'][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2647539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['documents'][0] == data['documents'][2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244774b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database._collection.count()\n",
    "database._collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddefceb3",
   "metadata": {},
   "source": [
    "# ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f132e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def get_embeddings(llm_type:str):\n",
    "\n",
    "    if llm_type == 'openai':\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    elif llm_type == 'upstage':\n",
    "        embeddings = UpstageEmbeddings(model=\"embedding-query\")\n",
    "    else:\n",
    "        st.error(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM íƒ€ì…ì…ë‹ˆë‹¤.\")\n",
    "        embeddings = None\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings('upstage')    \n",
    "database = Chroma(persist_directory='./public_db' , embedding_function=embeddings, collection_name='menu')\n",
    "retrieved_docs = database.similarity_search('í€ë“œë³„ì„±ê³¼ë¶„ì„ í™”ë©´', k=2)\n",
    "\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "\n",
    "# for id,doc in enumerate(retrieved_docs):\n",
    "#     print('-'*100)\n",
    "#     print(f'{id}ë²ˆì§¸ ë¬¸ì„œ')\n",
    "#     print(doc.page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f6d118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì´ ì•±ì€ ì‹ í•œí€ë“œíŒŒíŠ¸ë„ˆìŠ¤ë¼ëŠ” íšŒì‚¬ì˜ ì•±ì´ì•¼.\\nì‹ í•œí€ë“œíŒŒíŠ¸ë„ˆìŠ¤ëŠ” í•œêµ­ì˜ ì¼ë°˜ì‚¬ë¬´ê´€ë¦¬íšŒì‚¬ì•¼.\\nì¼ë°˜ì‚¬ë¬´ê´€ë¦¬íšŒì‚¬ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì™€ ê°™ì•„.\\nì¼ë°˜ì‚¬ë¬´ê´€ë¦¬íšŒì‚¬ëŠ” í•œêµ­ê±°ë˜ì†Œì— ìƒì¥ëœ ì£¼ì‹ì´ë‚˜ ì±„ê¶Œ ë“± ê¸ˆìœµíˆ¬ììƒí’ˆì˜ ë°œí–‰ì¸ìœ¼ë¡œë¶€í„° ìœ„íƒë°›ì€ ì£¼ì‹ ì‚¬ë¬´ ê´€ë ¨ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” íšŒì‚¬ë‹¤.\\nì¦‰, íˆ¬ììë“¤ì´ ì›í™œí•˜ê²Œ ê±°ë˜í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” ê²ƒì´ ì£¼ ëª©ì ì´ë‹¤.\\nì˜ˆë¥¼ ë“¤ì–´ ì£¼ì£¼ì´íšŒ ì†Œì§‘ í†µì§€ì„œ ë°œì†¡, ì¬ë¬´ì œí‘œ ìŠ¹ì¸ ê°™ì€ ì¤‘ìš”í•œ ì˜ì‚¬ ê²°ì • ì‹œ í•„ìš”í•œ ì„œë¥˜ ì •ë¦¬ë‚˜ ê³µì‹œë„ ë‹´ë‹¹í•œë‹¤.\\në˜í•œ ì¦ê¶Œì‚¬ì˜ ì£¼ë¬¸ ë‚´ì—­ í™•ì¸ í›„ ì´ë¥¼ í•´ë‹¹ ì¢…ëª©ì˜ ë§¤ë§¤ ì‹œìŠ¤í…œì— ì…ë ¥í•˜ê¸°ë„ í•œë‹¤.\\nì´ ì™¸ì—ë„ ëª…ì˜ê°œì„œ ëŒ€í–‰ì—…ë¬´, ë°°ë‹¹ê¸ˆ ì§€ê¸‰ ì¤€ë¹„ ì ˆì°¨ë“± ë‹¤ì–‘í•œ ë¶€ìˆ˜ì ì¸ ì¼ë“¤ì„ í•¨ê»˜ ì§„í–‰í•˜ë©° ë‹¨ìˆœíˆ ë¬¸ì„œì‘ì—… ë¿ ì•„ë‹ˆë¼ íš¨ìœ¨ì ì´ê³  ì •í™•í•œ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ê°€ í•„ìˆ˜ì ì´ë‹¤.\\në”°ë¼ì„œ ì´ëŸ¬í•œ ëª¨ë“  í™œë™ë“¤ì€ ì¼ì •í•œ ê·œì • ë‚´ì—ì„œ ì´ë£¨ì–´ì ¸ì•¼ í•˜ë©° ë²•ì  ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì² ì €í•œ ê²€í†  ê³¼ì •ë„ ê±°ì³ì•¼ í•œë‹¤.\\nì´ ì•±ì€ ì‹ í•œí€ë“œíŒŒíŠ¸ë„ˆìŠ¤ì— ì‚¬ë¬´ì—…ë¬´ë¥¼ ìœ„íƒí•œ ê³ ê°ì‚¬ì˜ í€ë“œë“¤ì„ ê´€ë¦¬í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ê³  ìˆì–´.\\n\\n\\nì•„ë˜ëŠ” í˜„ì¬ ì•±ì— ìˆëŠ” ë©”ë‰´ë“¤ì˜ ì •ë³´ì…ë‹ˆë‹¤.\\n1. ë‚´ë¶€ìˆ˜ìµë¥     : í€ë“œì˜ ë‚´ë¶€ ìˆ˜ìµë¥  ë˜ëŠ” IRRì„ ê³„ì‚°í•´ì£¼ëŠ” í™”ë©´. í€ë“œì˜ í˜„ê¸ˆíë¦„ì„ ê³„ì‚°í•˜ì—¬ ë‚´ë¶€ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•´ì£¼ëŠ” í™”ë©´. \\n2. í€ë“œì„±ê³¼ë¶„ì„   : í€ë“œì˜ ì„±ê³¼ë¥¼ ë¶„ì„í•´ì£¼ëŠ” í™”ë©´. í€ë“œì˜ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•˜ì—¬ í€ë“œì˜ ì„±ê³¼ë¥¼ ë¶„ì„í•´ì£¼ëŠ” í™”ë©´. ëˆ„ì ìˆ˜ìµë¥ , ì´ˆê³¼ìˆ˜ìµë¥ , BM ìˆ˜ìµë¥  ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ëŠ” í™”ë©´. \\n3. ì£¼ì‹ë³´ìœ í˜„í™©   : ì£¼ì‹ì˜ ë³´ìœ í˜„í™©ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. ì£¼ì‹ì˜ ë³´ìœ í˜„í™©ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. \\n4. ì±„ê¶Œë³´ìœ í˜„í™©   : ì±„ê¶Œì˜ ë³´ìœ í˜„í™©ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. ì±„ê¶Œì˜ ë³´ìœ í˜„í™©ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. \\n5. ì¢…í•©ë³´ìœ í˜„í™©   : ì£¼ì‹ê³¼ ì±„ê¶Œì˜ ë³´ìœ í˜„í™©ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. ì£¼ì‹ê³¼ ì±„ê¶Œì˜ ë³´ìœ í˜„í™©ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. \\n6. ì£¼ì‹ê±°ë˜ë‚´ì—­ : ì£¼ì‹ì˜ ê±°ë˜ë‚´ì—­ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. ì£¼ì‹ì˜ ê±°ë˜ë‚´ì—­ì„ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. \\n7. ê·¸ë˜í”„       : ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ëŠ” í™”ë©´. ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ëŠ” í™”ë©´. ìš´ìš©ì‚¬ ì§ì›ì´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê·¸ë˜í”„ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” í™”ë©´. \\n8. ìš´ìš©ì—­ì •ë³´   : í€ë“œ ìš´ìš©ì—­ì˜ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. ìš´ìš©ì—­ì˜ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. í€ë“œë§¤ë‹ˆì €ì˜ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” í™”ë©´.\\n9. í€ë“œê¸°ë³¸ì •ë³´ : í€ë“œì˜ ê¸°ë³¸ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ëŠ” í™”ë©´. í€ë“œì˜ ê¸°ë³¸ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ëŠ” í™”ë©´..1. êµ¬ì„±ì› ê°„ ì´ìµì˜ ë¶„ë°°ë¹„ìœ¨ì´ ì •í•˜ì—¬ì ¸ ìˆê³  í•´ë‹¹ êµ¬ì„±ì›ë³„ë¡œ ì´ìµì˜ ë¶„ë°°ë¹„ìœ¨ì´ í™•ì¸ë˜ëŠ” ê²½ìš°\\n\\n2. êµ¬ì„±ì› ê°„ ì´ìµì˜ ë¶„ë°°ë¹„ìœ¨ì´ ì •í•˜ì—¬ì ¸ ìˆì§€ ì•„ë‹ˆí•˜ë‚˜ ì‚¬ì‹¤ìƒ êµ¬ì„±ì›ë³„ë¡œ ì´ìµì´ ë¶„ë°°ë˜ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ë˜ëŠ” ê²½ìš°\\n\\nâ‘£ ì œ3í•­ì—ë„ ë¶ˆêµ¬í•˜ê³  í•´ë‹¹ ë‹¨ì²´ì˜ ì „ì²´ êµ¬ì„±ì› ì¤‘ ì¼ë¶€ êµ¬ì„±ì›ì˜ ë¶„ë°°ë¹„ìœ¨ë§Œ í™•ì¸ë˜ê±°ë‚˜ ì¼ë¶€ êµ¬ì„±ì›ì—ê²Œë§Œ ì´ìµì´ ë¶„ë°°ë˜ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ë˜ëŠ” ê²½ìš°ì—ëŠ” ë‹¤ìŒ ê° í˜¸ì˜ êµ¬ë¶„ì— ë”°ë¼ ì†Œë“ì„¸ ë˜ëŠ” ë²•ì¸ì„¸ë¥¼ ë‚©ë¶€í•  ì˜ë¬´ë¥¼ ì§„ë‹¤.<ì‹ ì„¤ 2018. 12. 31.>\\n\\n1. í™•ì¸ë˜ëŠ” ë¶€ë¶„: í•´ë‹¹ êµ¬ì„±ì›ë³„ë¡œ ì†Œë“ì„¸ ë˜ëŠ” ë²•ì¸ì„¸ì— ëŒ€í•œ ë‚©ì„¸ì˜ë¬´ ë¶€ë‹´\\n\\n2. í™•ì¸ë˜ì§€ ì•„ë‹ˆí•˜ëŠ” ë¶€ë¶„: í•´ë‹¹ ë‹¨ì²´ë¥¼ 1ê±°ì£¼ì ë˜ëŠ” 1ë¹„ê±°ì£¼ìë¡œ ë³´ì•„ ì†Œë“ì„¸ì— ëŒ€í•œ ë‚©ì„¸ì˜ë¬´ ë¶€ë‹´\\n\\nâ‘¤ ì œ3í•­ ë° ì œ4í•­ì—ë„ ë¶ˆêµ¬í•˜ê³  ë²•ì¸ìœ¼ë¡œ ë³´ëŠ” ë‹¨ì²´ ì™¸ì˜ ë²•ì¸ ì•„ë‹Œ ë‹¨ì²´ì— í•´ë‹¹í•˜ëŠ” êµ­ì™¸íˆ¬ìê¸°êµ¬(íˆ¬ìê¶Œìœ ë¥¼ í•˜ì—¬ ëª¨ì€ ê¸ˆì „ ë“±ì„ ê°€ì§€ê³  ì¬ì‚°ì  ê°€ì¹˜ê°€ ìˆëŠ” íˆ¬ìëŒ€ìƒìì‚°ì„ ì·¨ë“, ì²˜ë¶„í•˜ê±°ë‚˜ ê·¸ ë°–ì˜ ë°©ë²•ìœ¼ë¡œ ìš´ìš©í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ íˆ¬ììì—ê²Œ ë°°ë¶„í•˜ì—¬ ê·€ì†ì‹œí‚¤ëŠ” íˆ¬ìí–‰ìœ„ë¥¼ í•˜ëŠ” ê¸°êµ¬ë¡œì„œ êµ­ì™¸ì—ì„œ ì„¤ë¦½ëœ ê¸°êµ¬ë¥¼ ë§í•œë‹¤. ì´í•˜ ê°™ë‹¤)ë¥¼ ì œ119ì¡°ì˜2ì œ1í•­ì œ2í˜¸ì— ë”°ë¼ êµ­ë‚´ì›ì²œì†Œë“ì˜ ì‹¤ì§ˆê·€ì†ìë¡œ ë³´ëŠ” ê²½ìš° ê·¸ êµ­ì™¸íˆ¬ìê¸°êµ¬ëŠ” 1ë¹„ê±°ì£¼ìë¡œì„œ ì†Œë“ì„¸ë¥¼ ë‚©ë¶€í•  ì˜ë¬´ë¥¼ \\n\\në²•ì œì²˜\\n\\n\\n\\n2\\n\\nêµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs ='.'.join([data.page_content for data in retrieved_docs]) \n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25195cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8db569",
   "metadata": {},
   "source": [
    "# upstage basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e416a363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a large language model, so I don't have feelings, but I'm here and ready to help you. How can I assist you today? ğŸ˜Š  \n",
      "\n",
      "Is there something specific you'd like to talk about or ask? Whether it's questions, creative writing, problem-solving, or just casual conversationâ€”I'm here for it!  \n",
      "\n",
      "(And if you're feeling curious, I can share some fun facts or interesting tidbits too!)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI # openai==1.52.2\n",
    " \n",
    "client = OpenAI(\n",
    "    api_key=\"up_gxDYOP6oifTV8HSRaLoPJ3sUOixLE\",\n",
    "    base_url=\"https://api.upstage.ai/v1\"\n",
    ")\n",
    " \n",
    "stream = client.chat.completions.create(\n",
    "    model=\"solar-pro2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, how are you?\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    " \n",
    "# for chunk in stream:\n",
    "#     if chunk.choices[0].delta.content is not None:\n",
    "#         print(chunk.choices[0].delta.content, end=\"\")\n",
    " \n",
    "# Use with stream=False\n",
    "print(stream.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd55ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am **Upstage's Solar Pro 2**, a large language model (LLM) developed by **Upstage**, a leading AI company founded in 2020. Hereâ€™s a quick overview of who I am:\n",
      "\n",
      "### ğŸ“Œ **Key Features:**\n",
      "- **Model Size:** 30.9B parameters (Solar Pro 2 2025 version).\n",
      "- **Capabilities:**  \n",
      "  - **Thinking mode** (deep reasoning, complex tasks)  \n",
      "  - **Non-thinking mode** (fast, efficient responses)  \n",
      "  - Supports **Korean, Japanese, English**, and more.  \n",
      "  - Excels in **text generation, summarization, translation, coding**, and **structured document understanding (Document AI)**.\n",
      "\n",
      "### ğŸš€ **Powered by Upstage:**\n",
      "- **Technology:** Hybrid thinking modes, optimized for both performance and speed.  \n",
      "- **Document AI:** Specialized in **OCR, KIE (Key Information Extraction), and parsing structured documents**.  \n",
      "- **Solar API:** Offers **Chat, Structured Outputs, Translation, Summarization**, and **Code Generation** via APIs.  \n",
      "\n",
      "### ğŸŒ **Global Reach:**\n",
      "- Headquartered in **South Korea**, with offices in **Seoul, Tokyo, and Houston (Texas, U.S.)**.  \n",
      "- Partnered with **AWS** for cloud integration and global scalability.  \n",
      "\n",
      "Whether you need help with **research, writing, coding, or document processing**, Iâ€™m here to assist! ğŸš€  \n",
      "\n",
      "**How can I help you today?**\n"
     ]
    }
   ],
   "source": [
    " \n",
    "stream = client.chat.completions.create(\n",
    "    model=\"solar-pro2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"who are you?\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False,\n",
    ")\n",
    " \n",
    "print(stream.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77436474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import openai\n",
    "import os\n",
    "from common.gpt_learning_data import *\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "from common.global_data import support_llm_type\n",
    "load_dotenv()\n",
    "\n",
    "def draw_chatbot(key:str):\n",
    "\n",
    "    option = st.radio(\"LLM ëª¨ë¸ ì„ íƒ\",support_llm_type , horizontal=True)\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    if query := st.chat_input(\"ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\",key=key+\"_chat_input\"):\n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(query)\n",
    "        \n",
    "        try:                \n",
    "            if option == 'gpt-3.5-turbo(ìœ ë£Œ)' :\n",
    "                check_point = 1\n",
    "\n",
    "                # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "                client = openai.OpenAI(api_key=os.getenv(\"OPEN_AI_KEY\"))\n",
    "                \n",
    "                with st.spinner(\"ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...\"):\n",
    "                    # ì±„íŒ… ì™„ë£Œ ìš”ì²­\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"},\n",
    "                            *st.session_state.messages\n",
    "                        ],\n",
    "                        max_tokens=1000,\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "                \n",
    "                response = response.choices[0].message.content\n",
    "\n",
    "            elif option == 'upstage(ìœ ë£Œ)':\n",
    "                check_point = 2\n",
    "                \n",
    "                client = openai.OpenAI(api_key=os.getenv(\"UPSTAGE_API_KEY\") ,base_url=\"https://api.upstage.ai/v1\")\n",
    "                \n",
    "                with st.spinner(\"ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...\"):\n",
    "                    \n",
    "                    # ìœ ì‚¬ë„ê²€ìƒ‰ \n",
    "                    retrieved_data = database.similarity_search(query, k=3)  # këŠ” ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê°€ì ¸ì˜¬ ë¬¸ì„œì˜ ê°œìˆ˜\n",
    "                    # ì§ˆì˜ \n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"solar-pro2\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"},\n",
    "                            *gpt_data,\n",
    "                            *st.session_state.messages\n",
    "                        ],\n",
    "                        stream=False,\n",
    "                    )\n",
    "                \n",
    "                response = response.choices[0].message.content\n",
    "\n",
    "            # ì¶”í›„ ë” ë‚˜ëŠ” ë¡œì»¬ llm  ìœ¼ë¡œ ê°œì„  (ì†ë„ ë„ˆë¬´ ëŠë¦¼)\n",
    "            # elif option == 'exaone3.5(ë¬´ë£Œ)':\n",
    "            #     check_point = 3\n",
    "            #     with st.spinner(\"ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...\"):\n",
    "            #         llm = ChatOllama(model ='exaone3.5:2.4b', verbose=True)                    \n",
    "            #         chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            #             (\"system\",\"ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ëŠ” ì¹œì ˆí•œ ë„ìš°ë¯¸ì•¼. ë‹¹ì‹ ì€ ë‹¤ìŒì˜ ì •ë³´ë“¤ì„ ì•Œê³  ìˆì–´. í™”ë©´ì„ ì°¾ëŠ” ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì¤˜.\" + app_explain + menu_explain),\n",
    "            #             (\"human\", \"{question} 3ë¬¸ì¥ ì´ë‚´ë¡œ ëŒ€ë‹µí•´ì¤˜\")\n",
    "            #         ]\n",
    "            #         )                    \n",
    "            #         menu_chain ={\"question\" : RunnablePassthrough()} |chat_prompt_template | llm | StrOutputParser()\n",
    "            #         response = menu_chain.invoke(query)\n",
    "\n",
    "        except Exception as e:\n",
    "            if check_point == 1:\n",
    "                response = f\"âŒ API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "            elif check_point == 2:\n",
    "                response =f\"âŒ API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "\n",
    "        # ë´‡ ì‘ë‹µ í‘œì‹œ\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.write(response)        \n",
    "        # ë´‡ ì‘ë‹µ ì¶”ê°€\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "    def clear_chat():\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "    if st.session_state.messages:\n",
    "        st.button(\"ì±„íŒ… ì´ˆê¸°í™”\", on_click=clear_chat,key=key+\"_chat_clear_btn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12086f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHI\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
